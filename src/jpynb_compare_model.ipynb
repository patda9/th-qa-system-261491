{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# utils\n",
    "import datetime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# numerical libs\n",
    "import numpy as np\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import BatchNormalization, Bidirectional, Concatenate, Dense, Flatten, GRU, Input, Lambda, LSTM, Masking, multiply, Permute, RepeatVector\n",
    "from keras.models import load_model, Model, Sequential\n",
    "\n",
    "# seed and precision\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# file i/o paths\n",
    "q_path = '../data/dataset/questions/embedded_questions_4000_40_300.npy'\n",
    "n0_path = 'D:/Users/Patdanai/th-qasys-db/n0_embedded/n0_embedded/'\n",
    "n1_path = 'D:/Users/Patdanai/th-qasys-db/n1_embedded/n1_embedded/'\n",
    "p_path = 'D:/Users/Patdanai/th-qasys-db/positive_embedded/positive_embedded/'\n",
    "dataset_paths = [p_path, n0_path, n1_path, q_path]\n",
    "\n",
    "# model hyperparameters\n",
    "hidden_nodes = 16\n",
    "rnn_units = 64\n",
    "sl = 40\n",
    "wvl = 300\n",
    "\n",
    "def attention_layer(inputs, time_step):\n",
    "    a = Lambda(lambda x: x, output_shape=lambda s: s)(inputs)\n",
    "    a = Permute((2, 1))(a)\n",
    "    a = Dense(time_step, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    output_attention = multiply([inputs, a_probs], name='multiply_attention')\n",
    "    return output_attention\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    beta = 1\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "def get_input(input_path):\n",
    "    return np.load(input_path)\n",
    "\n",
    "def get_dataset_set(dataset_paths, batch_size=4000, samples_per_file=5):\n",
    "    if(samples_per_file > 5 or not samples_per_file):\n",
    "        print('samples per file must be in range [0, 5]')\n",
    "        return\n",
    "\n",
    "    question_sentences = get_input(dataset_paths[3])\n",
    "    print(question_sentences.shape)\n",
    "\n",
    "    files = np.random.permutation(os.listdir(dataset_paths[0]))\n",
    "    \n",
    "    s_seq = (0, sl, wvl)\n",
    "    l_shape = (0, 1)\n",
    "\n",
    "    questions_batch = np.empty((s_seq))\n",
    "    sentences_batch = np.empty((s_seq))\n",
    "    labels_batch = np.empty((l_shape))\n",
    "\n",
    "    f_count = 0\n",
    "    for f_name in files:\n",
    "        if(f_count > batch_size - 1):\n",
    "            break\n",
    "\n",
    "        q_idx = f_name.replace('positive_question', '').replace('.npy', '')\n",
    "\n",
    "        q = question_sentences[int(q_idx)]\n",
    "        q = np.expand_dims(q, axis=0)\n",
    "\n",
    "        positive_s = get_input(dataset_paths[0] + f_name)[:samples_per_file]\n",
    "        l = np.ones((positive_s.shape[0], 1))\n",
    "        labels_batch = np.concatenate((labels_batch, l), axis=0)\n",
    "\n",
    "        negative_s = np.empty((s_seq))\n",
    "        if(samples_per_file < 2):\n",
    "            r = np.random.randint(2)\n",
    "            if(r):\n",
    "                negative0_s = get_input(dataset_paths[1] + 'negative0_question%s.npy' % q_idx)[:1]\n",
    "                negative_s = np.concatenate((negative_s, negative0_s), axis=0)\n",
    "            else:\n",
    "                negative1_s = get_input(dataset_paths[2] + 'negative1_question%s.npy' % q_idx)[:1]\n",
    "                negative_s = np.concatenate((negative_s, negative1_s), axis=0)\n",
    "        else:\n",
    "            n0_instance = samples_per_file // 2\n",
    "            negative0_s = get_input(dataset_paths[1] + 'negative0_question%s.npy' % q_idx)[:n0_instance]\n",
    "            n1_instance = samples_per_file - negative0_s.shape[0]\n",
    "            negative1_s = get_input(dataset_paths[2] + 'negative1_question%s.npy' % q_idx)[:n1_instance]\n",
    "            \n",
    "            temp = np.concatenate((negative0_s, negative1_s), axis=0)\n",
    "            negative_s = np.concatenate((negative_s, temp), axis=0)\n",
    "        \n",
    "        temp_s = np.concatenate((positive_s, negative_s), axis=0)\n",
    "        sentences_batch = np.concatenate((sentences_batch, temp_s), axis=0)\n",
    "        l = np.zeros((negative_s.shape[0], 1))\n",
    "        labels_batch = np.concatenate((labels_batch, l), axis=0)\n",
    "\n",
    "        temp_q = np.repeat(q, positive_s.shape[0] + negative_s.shape[0], axis=0)\n",
    "        questions_batch = np.concatenate((questions_batch, temp_q), axis=0)\n",
    "\n",
    "        f_count += 1\n",
    "        \n",
    "        if(f_count % 100):\n",
    "            print('file[%s/%s] => ps_shape = %s ns_shape = %s ps_ns_shape = %s l_shape = %s qs_shape = %s' % (f_count, batch_size, positive_s.shape, negative_s.shape, sentences_batch.shape, labels_batch.shape, questions_batch.shape))\n",
    "    \n",
    "    return ([questions_batch, sentences_batch], labels_batch)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, cmap=plt.cm.Blues,\n",
    "                        normalize=0, save_path=None, title=None, verbose=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting ``normalize=True``.\n",
    "    \"\"\"\n",
    "\n",
    "    if(not title):\n",
    "        if(normalize):\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if(normalize):\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    if(verbose):\n",
    "        print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='Actual',\n",
    "           xlabel='Predicted')\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if(save_path):\n",
    "        plt.savefig(save_path)\n",
    "        plt.clf()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    return ax\n",
    "\n",
    "def plot_training_history(h, labels=['Training', 'Validation'], yl=None, save_path=None):\n",
    "    plt.plot(h[0], label=labels[0])\n",
    "    plt.plot(h[1], '-r', label=labels[1])\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('Epochs')\n",
    "\n",
    "    if(yl):\n",
    "        plt.ylabel(yl)\n",
    "    \n",
    "    if(save_path):\n",
    "        plt.savefig(save_path)\n",
    "        plt.clf()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def sequence_generator(files, label_path, batch_size=512):\n",
    "    # files => os.listdir\n",
    "    if(label_path):\n",
    "        positive_dataset_path = label_path[0]\n",
    "        negative0_dataset_path = label_path[1]\n",
    "        negative1_dataset_path = label_path[2]\n",
    "    \n",
    "    while(1):\n",
    "        batch_paths = np.random.choice(files, size=batch_size)\n",
    "        input_batch = np.empty((0, 20, 300))\n",
    "        output_batch = np.empty((0, 1))\n",
    "\n",
    "        inp = None\n",
    "        for input_path in batch_paths:\n",
    "            if('positive' in input_path):\n",
    "                inp = get_input(positive_dataset_path + input_path)\n",
    "                out = np.ones((inp.shape[0], 1))\n",
    "            elif('negative0' in input_path):\n",
    "                inp = get_input(negative0_dataset_path + input_path)\n",
    "                out = np.zeros((inp.shape[0], 1))\n",
    "            elif('negative1' in input_path):\n",
    "                inp = get_input(negative1_dataset_path + input_path)\n",
    "                out = np.zeros((inp.shape[0], 1))\n",
    "\n",
    "            input_batch = np.concatenate((input_batch, inp), axis=0)\n",
    "            output_batch = np.concatenate((output_batch, out), axis=0)\n",
    "\n",
    "        input_batch = np.array(input_batch)\n",
    "        output_batch = np.array(output_batch)\n",
    "\n",
    "        yield (input_batch, output_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 40, 300)\n",
      "file[2048/2048] => ps_shape = (5, 40, 300) ns_shape = (5, 40, 300) ps_ns_shape = (20480, 40, 300) l_shape = (20480, 1) qs_shape = (20480, 40, 300)\n",
      "train on 16384 samples\n",
      "validate on 4096 samples\n"
     ]
    }
   ],
   "source": [
    "##   dataset\n",
    "batch_size = 2048\n",
    "samples_per_file = 5\n",
    "validation_proportion = .8\n",
    "\n",
    "# generate training set\n",
    "training_set = get_dataset_set(dataset_paths, batch_size=batch_size, samples_per_file=samples_per_file)\n",
    "\n",
    "training_size = int(len(training_set[1]) * validation_proportion)\n",
    "y_train = training_set[1][:training_size]\n",
    "x_train = [training_set[0][0][:training_size], training_set[0][1][:training_size]]\n",
    "\n",
    "y_test = training_set[1][training_size:]\n",
    "x_test = [training_set[0][0][training_size:], training_set[0][1][training_size:]]\n",
    "\n",
    "print('train on', len(y_train), 'samples')\n",
    "print('validate on', len(y_test), 'samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##    model fitting\n",
    "epochs = 128\n",
    "training_batch_size = 2048\n",
    "validation_split = .4\n",
    "\n",
    "def sentence_vector(recurrent_layer):\n",
    "    input_seq = Input(shape=(sl, wvl))\n",
    "    masking = Masking(mask_value=0., input_shape=(sl, wvl))(input_seq)\n",
    "    rl = recurrent_layer(masking)\n",
    "    multiply_attention = attention_layer(rl, sl)\n",
    "    multiply_attention = Lambda(lambda x: x, output_shape=lambda s: s)(multiply_attention)\n",
    "    multiply_attention = Flatten()(multiply_attention)\n",
    "    output = Dense(rnn_units, activation='relu')(multiply_attention)\n",
    "    submodel = Model(inputs=input_seq, outputs=output)\n",
    "    submodel.summary()\n",
    "    return submodel\n",
    "\n",
    "def sentence_compare():\n",
    "    qv = Input(shape=(rnn_units, ), name='vectorized_q')\n",
    "    sv = Input(shape=(rnn_units, ), name='vectorized_s')\n",
    "\n",
    "    concatenate = Concatenate()([qv, sv])\n",
    "    dense1 = Dense(hidden_nodes, activation='sigmoid')(concatenate)\n",
    "    dense2 = Dense(hidden_nodes, activation='sigmoid')(dense1)\n",
    "    similarity = Dense(1, activation='sigmoid')(dense2)\n",
    "    submodel = Model(inputs=[qv, sv], outputs=similarity)\n",
    "    submodel.summary()\n",
    "    return submodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 40, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_19 (Masking)            (None, 40, 300)      0           input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 40, 128)      186880      masking_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 40, 128)      0           bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "permute_37 (Permute)            (None, 128, 40)      0           lambda_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_64 (Dense)                (None, 128, 40)      1640        permute_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "permute_38 (Permute)            (None, 40, 128)      0           dense_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_attention (Multiply)   (None, 40, 128)      0           bidirectional_19[0][0]           \n",
      "                                                                 permute_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 40, 128)      0           multiply_attention[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 5120)         0           lambda_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_65 (Dense)                (None, 64)           327744      flatten_19[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 516,264\n",
      "Trainable params: 516,264\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           (None, 40, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_20 (Masking)            (None, 40, 300)      0           input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 40, 128)      186880      masking_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 40, 128)      0           bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "permute_39 (Permute)            (None, 128, 40)      0           lambda_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_66 (Dense)                (None, 128, 40)      1640        permute_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "permute_40 (Permute)            (None, 40, 128)      0           dense_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_attention (Multiply)   (None, 40, 128)      0           bidirectional_20[0][0]           \n",
      "                                                                 permute_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 40, 128)      0           multiply_attention[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 5120)         0           lambda_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_67 (Dense)                (None, 64)           327744      flatten_20[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 516,264\n",
      "Trainable params: 516,264\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "vectorized_q (InputLayer)       (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vectorized_s (InputLayer)       (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 128)          0           vectorized_q[0][0]               \n",
      "                                                                 vectorized_s[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_68 (Dense)                (None, 16)           2064        concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_69 (Dense)                (None, 16)           272         dense_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_70 (Dense)                (None, 1)            17          dense_69[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,353\n",
      "Trainable params: 2,353\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "question_seq (InputLayer)       (None, 40, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sentence_seq (InputLayer)       (None, 40, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question_vector (Model)         (None, 64)           516264      question_seq[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sentence_vector (Model)         (None, 64)           516264      sentence_seq[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "compare (Model)                 (None, 1)            2353        question_vector[1][0]            \n",
      "                                                                 sentence_vector[1][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,034,881\n",
      "Trainable params: 1,034,881\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create tensors \n",
    "q_seq = Input(shape=(sl, wvl), name='question_seq')\n",
    "s_seq = Input(shape=(sl, wvl), name='sentence_seq')\n",
    "\n",
    "# create recurrrent layers\n",
    "current_model = 'lstm'\n",
    "rl1 = Bidirectional(LSTM(rnn_units, activation='relu', dropout=.4, recurrent_dropout=.1, return_sequences=1))\n",
    "rl2 = Bidirectional(LSTM(rnn_units, activation='relu', dropout=.4, recurrent_dropout=.1, return_sequences=1))\n",
    "\n",
    "# create output tensors\n",
    "qv = sentence_vector(rl1)\n",
    "qv.name = 'question_vector'\n",
    "qv = qv(q_seq)\n",
    "\n",
    "sv = sentence_vector(rl2)\n",
    "sv.name = 'sentence_vector'\n",
    "sv = sv(s_seq)\n",
    "\n",
    "similarity = sentence_compare()\n",
    "similarity.name = 'compare'\n",
    "similarity = similarity([qv, sv])\n",
    "\n",
    "# form model\n",
    "model = Model(inputs=[q_seq, s_seq], outputs=similarity)\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# training configuration\n",
    "##    model confiuration\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['mae', 'acc'])\n",
    "\n",
    "##    load model\n",
    "# model = load_model('../model/trained_compare_model_2019_05_03_14_55_55.705298.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training process.\n",
      "Train on 9830 samples, validate on 6554 samples\n",
      "Epoch 1/128\n",
      "9830/9830 [==============================] - 15s 2ms/step - loss: 0.7060 - mean_absolute_error: 0.5000 - acc: 0.5000 - val_loss: 0.7015 - val_mean_absolute_error: 0.5000 - val_acc: 0.5003\n",
      "Epoch 2/128\n",
      "9830/9830 [==============================] - 6s 561us/step - loss: 0.6995 - mean_absolute_error: 0.5001 - acc: 0.5000 - val_loss: 0.6962 - val_mean_absolute_error: 0.5000 - val_acc: 0.5003\n",
      "Epoch 3/128\n",
      "9830/9830 [==============================] - 5s 558us/step - loss: 0.6950 - mean_absolute_error: 0.5000 - acc: 0.5000 - val_loss: 0.6936 - val_mean_absolute_error: 0.5000 - val_acc: 0.5003\n",
      "Epoch 4/128\n",
      "9830/9830 [==============================] - 5s 556us/step - loss: 0.6932 - mean_absolute_error: 0.5000 - acc: 0.5016 - val_loss: 0.6932 - val_mean_absolute_error: 0.5000 - val_acc: 0.4995\n",
      "Epoch 5/128\n",
      "9830/9830 [==============================] - 5s 557us/step - loss: 0.6933 - mean_absolute_error: 0.5000 - acc: 0.5000 - val_loss: 0.6935 - val_mean_absolute_error: 0.4999 - val_acc: 0.4997\n",
      "Epoch 6/128\n",
      "9830/9830 [==============================] - 5s 558us/step - loss: 0.6934 - mean_absolute_error: 0.4999 - acc: 0.5000 - val_loss: 0.6931 - val_mean_absolute_error: 0.4998 - val_acc: 0.4997\n",
      "Epoch 7/128\n",
      "9830/9830 [==============================] - 6s 565us/step - loss: 0.6929 - mean_absolute_error: 0.4997 - acc: 0.5000 - val_loss: 0.6924 - val_mean_absolute_error: 0.4995 - val_acc: 0.4997\n",
      "Epoch 8/128\n",
      "9830/9830 [==============================] - 5s 558us/step - loss: 0.6916 - mean_absolute_error: 0.4991 - acc: 0.5000 - val_loss: 0.6900 - val_mean_absolute_error: 0.4983 - val_acc: 0.5195\n",
      "Epoch 9/128\n",
      "9830/9830 [==============================] - 6s 560us/step - loss: 0.6878 - mean_absolute_error: 0.4968 - acc: 0.5387 - val_loss: 0.6852 - val_mean_absolute_error: 0.4953 - val_acc: 0.5604\n",
      "Epoch 10/128\n",
      "9830/9830 [==============================] - 6s 562us/step - loss: 0.6838 - mean_absolute_error: 0.4946 - acc: 0.6062 - val_loss: 0.6847 - val_mean_absolute_error: 0.4955 - val_acc: 0.6244\n",
      "Epoch 11/128\n",
      "9830/9830 [==============================] - 6s 561us/step - loss: 0.6823 - mean_absolute_error: 0.4941 - acc: 0.6307 - val_loss: 0.6800 - val_mean_absolute_error: 0.4929 - val_acc: 0.6396\n",
      "Epoch 12/128\n",
      "9830/9830 [==============================] - 6s 562us/step - loss: 0.6753 - mean_absolute_error: 0.4888 - acc: 0.6213 - val_loss: 0.6694 - val_mean_absolute_error: 0.4846 - val_acc: 0.6129\n",
      "Epoch 13/128\n",
      "9830/9830 [==============================] - 6s 565us/step - loss: 0.6685 - mean_absolute_error: 0.4839 - acc: 0.6237 - val_loss: 0.6651 - val_mean_absolute_error: 0.4835 - val_acc: 0.6405\n",
      "Epoch 14/128\n",
      "9830/9830 [==============================] - 6s 565us/step - loss: 0.6617 - mean_absolute_error: 0.4804 - acc: 0.6383 - val_loss: 0.6539 - val_mean_absolute_error: 0.4750 - val_acc: 0.6504\n",
      "Epoch 15/128\n",
      "9830/9830 [==============================] - 6s 564us/step - loss: 0.6559 - mean_absolute_error: 0.4756 - acc: 0.6411 - val_loss: 0.6493 - val_mean_absolute_error: 0.4726 - val_acc: 0.6518\n",
      "Epoch 16/128\n",
      "9830/9830 [==============================] - 6s 564us/step - loss: 0.6497 - mean_absolute_error: 0.4718 - acc: 0.6452 - val_loss: 0.6406 - val_mean_absolute_error: 0.4669 - val_acc: 0.6625\n",
      "Epoch 17/128\n",
      "9830/9830 [==============================] - 6s 566us/step - loss: 0.6414 - mean_absolute_error: 0.4668 - acc: 0.6580 - val_loss: 0.6315 - val_mean_absolute_error: 0.4609 - val_acc: 0.6707\n",
      "Epoch 18/128\n",
      "9830/9830 [==============================] - 6s 567us/step - loss: 0.6293 - mean_absolute_error: 0.4585 - acc: 0.6659 - val_loss: 0.6141 - val_mean_absolute_error: 0.4492 - val_acc: 0.6793\n",
      "Epoch 19/128\n",
      "9830/9830 [==============================] - 6s 566us/step - loss: 0.6142 - mean_absolute_error: 0.4469 - acc: 0.6724 - val_loss: 0.5947 - val_mean_absolute_error: 0.4364 - val_acc: 0.6875\n",
      "Epoch 20/128\n",
      "9830/9830 [==============================] - 6s 567us/step - loss: 0.6007 - mean_absolute_error: 0.4333 - acc: 0.6931 - val_loss: 0.5912 - val_mean_absolute_error: 0.4281 - val_acc: 0.7156\n",
      "Epoch 21/128\n",
      "9830/9830 [==============================] - 6s 570us/step - loss: 0.5787 - mean_absolute_error: 0.4162 - acc: 0.7271 - val_loss: 0.5521 - val_mean_absolute_error: 0.4032 - val_acc: 0.7528\n",
      "Epoch 22/128\n",
      "9830/9830 [==============================] - 6s 595us/step - loss: 0.5596 - mean_absolute_error: 0.4062 - acc: 0.7506 - val_loss: 0.5418 - val_mean_absolute_error: 0.3976 - val_acc: 0.7661\n",
      "Epoch 23/128\n",
      "9830/9830 [==============================] - 6s 596us/step - loss: 0.5396 - mean_absolute_error: 0.3932 - acc: 0.7636 - val_loss: 0.5254 - val_mean_absolute_error: 0.3866 - val_acc: 0.7646\n",
      "Epoch 24/128\n",
      "9830/9830 [==============================] - 6s 589us/step - loss: 0.5277 - mean_absolute_error: 0.3858 - acc: 0.7670 - val_loss: 0.5087 - val_mean_absolute_error: 0.3785 - val_acc: 0.7943\n",
      "Epoch 25/128\n",
      "9830/9830 [==============================] - 6s 628us/step - loss: 0.5079 - mean_absolute_error: 0.3723 - acc: 0.7883 - val_loss: 0.4831 - val_mean_absolute_error: 0.3564 - val_acc: 0.8120\n",
      "Epoch 26/128\n",
      "9830/9830 [==============================] - 6s 588us/step - loss: 0.4911 - mean_absolute_error: 0.3561 - acc: 0.7981 - val_loss: 0.4775 - val_mean_absolute_error: 0.3514 - val_acc: 0.8087\n",
      "Epoch 27/128\n",
      "9830/9830 [==============================] - 6s 574us/step - loss: 0.4834 - mean_absolute_error: 0.3510 - acc: 0.7990 - val_loss: 0.4564 - val_mean_absolute_error: 0.3389 - val_acc: 0.8190\n",
      "Epoch 28/128\n",
      "9830/9830 [==============================] - 6s 572us/step - loss: 0.4635 - mean_absolute_error: 0.3377 - acc: 0.8097 - val_loss: 0.4485 - val_mean_absolute_error: 0.3305 - val_acc: 0.8181\n",
      "Epoch 29/128\n",
      "9830/9830 [==============================] - 6s 569us/step - loss: 0.4552 - mean_absolute_error: 0.3280 - acc: 0.8104 - val_loss: 0.4338 - val_mean_absolute_error: 0.3165 - val_acc: 0.8218\n",
      "Epoch 30/128\n",
      "9830/9830 [==============================] - 6s 572us/step - loss: 0.4420 - mean_absolute_error: 0.3171 - acc: 0.8156 - val_loss: 0.4291 - val_mean_absolute_error: 0.3132 - val_acc: 0.8227\n",
      "Epoch 31/128\n",
      "9830/9830 [==============================] - 6s 580us/step - loss: 0.4287 - mean_absolute_error: 0.3097 - acc: 0.8237 - val_loss: 0.4184 - val_mean_absolute_error: 0.3051 - val_acc: 0.8277\n",
      "Epoch 32/128\n",
      "9830/9830 [==============================] - 6s 572us/step - loss: 0.4224 - mean_absolute_error: 0.3037 - acc: 0.8220 - val_loss: 0.4124 - val_mean_absolute_error: 0.2995 - val_acc: 0.8265\n",
      "Epoch 33/128\n",
      "9830/9830 [==============================] - 6s 571us/step - loss: 0.4123 - mean_absolute_error: 0.2959 - acc: 0.8316 - val_loss: 0.4051 - val_mean_absolute_error: 0.2930 - val_acc: 0.8259\n",
      "Epoch 34/128\n",
      "9830/9830 [==============================] - 6s 571us/step - loss: 0.4061 - mean_absolute_error: 0.2900 - acc: 0.8280 - val_loss: 0.4000 - val_mean_absolute_error: 0.2877 - val_acc: 0.8293\n",
      "Epoch 35/128\n",
      "9830/9830 [==============================] - 6s 570us/step - loss: 0.3953 - mean_absolute_error: 0.2819 - acc: 0.8360 - val_loss: 0.3942 - val_mean_absolute_error: 0.2817 - val_acc: 0.8305\n",
      "Epoch 36/128\n",
      "9830/9830 [==============================] - 6s 574us/step - loss: 0.3885 - mean_absolute_error: 0.2759 - acc: 0.8388 - val_loss: 0.3887 - val_mean_absolute_error: 0.2772 - val_acc: 0.8334\n",
      "Epoch 37/128\n",
      "9830/9830 [==============================] - 6s 569us/step - loss: 0.3814 - mean_absolute_error: 0.2705 - acc: 0.8430 - val_loss: 0.3870 - val_mean_absolute_error: 0.2744 - val_acc: 0.8308\n",
      "Epoch 38/128\n",
      "9830/9830 [==============================] - 6s 569us/step - loss: 0.3696 - mean_absolute_error: 0.2637 - acc: 0.8490 - val_loss: 0.3827 - val_mean_absolute_error: 0.2717 - val_acc: 0.8364\n",
      "Epoch 39/128\n",
      "9830/9830 [==============================] - 6s 573us/step - loss: 0.3641 - mean_absolute_error: 0.2596 - acc: 0.8497 - val_loss: 0.3797 - val_mean_absolute_error: 0.2682 - val_acc: 0.8312\n",
      "Epoch 40/128\n",
      "9830/9830 [==============================] - 6s 574us/step - loss: 0.3591 - mean_absolute_error: 0.2553 - acc: 0.8528 - val_loss: 0.3769 - val_mean_absolute_error: 0.2633 - val_acc: 0.8303\n",
      "Epoch 41/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9830/9830 [==============================] - 6s 570us/step - loss: 0.3515 - mean_absolute_error: 0.2481 - acc: 0.8565 - val_loss: 0.3725 - val_mean_absolute_error: 0.2589 - val_acc: 0.8404\n",
      "Epoch 42/128\n",
      "9830/9830 [==============================] - 6s 582us/step - loss: 0.3477 - mean_absolute_error: 0.2437 - acc: 0.8556 - val_loss: 0.3683 - val_mean_absolute_error: 0.2547 - val_acc: 0.8341\n",
      "Epoch 43/128\n",
      "9830/9830 [==============================] - 6s 567us/step - loss: 0.3379 - mean_absolute_error: 0.2372 - acc: 0.8627 - val_loss: 0.3700 - val_mean_absolute_error: 0.2525 - val_acc: 0.8308\n",
      "Epoch 44/128\n",
      "9830/9830 [==============================] - 6s 572us/step - loss: 0.3321 - mean_absolute_error: 0.2330 - acc: 0.8660 - val_loss: 0.3638 - val_mean_absolute_error: 0.2495 - val_acc: 0.8377\n",
      "Epoch 45/128\n",
      "9830/9830 [==============================] - 6s 567us/step - loss: 0.3274 - mean_absolute_error: 0.2302 - acc: 0.8682 - val_loss: 0.3622 - val_mean_absolute_error: 0.2482 - val_acc: 0.8370\n",
      "Epoch 46/128\n",
      "9830/9830 [==============================] - 6s 599us/step - loss: 0.3211 - mean_absolute_error: 0.2261 - acc: 0.8701 - val_loss: 0.3630 - val_mean_absolute_error: 0.2458 - val_acc: 0.8364\n",
      "Epoch 47/128\n",
      "9830/9830 [==============================] - 6s 571us/step - loss: 0.3160 - mean_absolute_error: 0.2221 - acc: 0.8712 - val_loss: 0.3597 - val_mean_absolute_error: 0.2429 - val_acc: 0.8349\n",
      "Epoch 48/128\n",
      "9830/9830 [==============================] - 6s 568us/step - loss: 0.3094 - mean_absolute_error: 0.2169 - acc: 0.8749 - val_loss: 0.3569 - val_mean_absolute_error: 0.2394 - val_acc: 0.8351\n",
      "Epoch 49/128\n",
      "9830/9830 [==============================] - 6s 571us/step - loss: 0.3065 - mean_absolute_error: 0.2134 - acc: 0.8772 - val_loss: 0.3580 - val_mean_absolute_error: 0.2374 - val_acc: 0.8335\n",
      "Epoch 50/128\n",
      "9830/9830 [==============================] - 6s 581us/step - loss: 0.3010 - mean_absolute_error: 0.2081 - acc: 0.8797 - val_loss: 0.3571 - val_mean_absolute_error: 0.2343 - val_acc: 0.8361\n",
      "Epoch 51/128\n",
      "9830/9830 [==============================] - 6s 582us/step - loss: 0.2992 - mean_absolute_error: 0.2045 - acc: 0.8816 - val_loss: 0.3518 - val_mean_absolute_error: 0.2308 - val_acc: 0.8404\n",
      "Epoch 52/128\n",
      "9830/9830 [==============================] - 6s 588us/step - loss: 0.2981 - mean_absolute_error: 0.2035 - acc: 0.8785 - val_loss: 0.3496 - val_mean_absolute_error: 0.2296 - val_acc: 0.8370\n",
      "Epoch 53/128\n",
      "9830/9830 [==============================] - 6s 572us/step - loss: 0.2885 - mean_absolute_error: 0.1992 - acc: 0.8854 - val_loss: 0.3509 - val_mean_absolute_error: 0.2296 - val_acc: 0.8366\n",
      "Epoch 54/128\n",
      "9830/9830 [==============================] - 6s 571us/step - loss: 0.2849 - mean_absolute_error: 0.1968 - acc: 0.8877 - val_loss: 0.3617 - val_mean_absolute_error: 0.2289 - val_acc: 0.8305\n",
      "Epoch 55/128\n",
      "9830/9830 [==============================] - 6s 574us/step - loss: 0.2825 - mean_absolute_error: 0.1934 - acc: 0.8871 - val_loss: 0.3488 - val_mean_absolute_error: 0.2243 - val_acc: 0.8381\n",
      "Epoch 56/128\n",
      "9830/9830 [==============================] - 6s 572us/step - loss: 0.2777 - mean_absolute_error: 0.1902 - acc: 0.8895 - val_loss: 0.3490 - val_mean_absolute_error: 0.2232 - val_acc: 0.8396\n",
      "Epoch 57/128\n",
      "9830/9830 [==============================] - 6s 573us/step - loss: 0.2743 - mean_absolute_error: 0.1862 - acc: 0.8898 - val_loss: 0.3494 - val_mean_absolute_error: 0.2211 - val_acc: 0.8395\n",
      "Epoch 58/128\n",
      "9830/9830 [==============================] - 6s 590us/step - loss: 0.2715 - mean_absolute_error: 0.1834 - acc: 0.8921 - val_loss: 0.3527 - val_mean_absolute_error: 0.2204 - val_acc: 0.8340\n",
      "Epoch 59/128\n",
      "9830/9830 [==============================] - 6s 587us/step - loss: 0.2633 - mean_absolute_error: 0.1793 - acc: 0.8966 - val_loss: 0.3554 - val_mean_absolute_error: 0.2201 - val_acc: 0.8328\n",
      "Epoch 60/128\n",
      "9830/9830 [==============================] - 6s 575us/step - loss: 0.2590 - mean_absolute_error: 0.1771 - acc: 0.9000 - val_loss: 0.3492 - val_mean_absolute_error: 0.2175 - val_acc: 0.8377\n",
      "Epoch 61/128\n",
      "9830/9830 [==============================] - 6s 582us/step - loss: 0.2548 - mean_absolute_error: 0.1733 - acc: 0.9035 - val_loss: 0.3467 - val_mean_absolute_error: 0.2149 - val_acc: 0.8409\n",
      "Epoch 62/128\n",
      "9830/9830 [==============================] - 6s 596us/step - loss: 0.2528 - mean_absolute_error: 0.1710 - acc: 0.9031 - val_loss: 0.3460 - val_mean_absolute_error: 0.2131 - val_acc: 0.8387\n",
      "Epoch 63/128\n",
      "9830/9830 [==============================] - 6s 583us/step - loss: 0.2473 - mean_absolute_error: 0.1669 - acc: 0.9028 - val_loss: 0.3446 - val_mean_absolute_error: 0.2122 - val_acc: 0.8410\n",
      "Epoch 64/128\n",
      "9830/9830 [==============================] - 6s 589us/step - loss: 0.2420 - mean_absolute_error: 0.1652 - acc: 0.9073 - val_loss: 0.3478 - val_mean_absolute_error: 0.2123 - val_acc: 0.8345\n",
      "Epoch 65/128\n",
      "9830/9830 [==============================] - 6s 583us/step - loss: 0.2349 - mean_absolute_error: 0.1617 - acc: 0.9092 - val_loss: 0.3422 - val_mean_absolute_error: 0.2095 - val_acc: 0.8447\n",
      "Epoch 66/128\n",
      "9830/9830 [==============================] - 6s 577us/step - loss: 0.2340 - mean_absolute_error: 0.1588 - acc: 0.9124 - val_loss: 0.3422 - val_mean_absolute_error: 0.2064 - val_acc: 0.8430\n",
      "Epoch 67/128\n",
      "9830/9830 [==============================] - 6s 574us/step - loss: 0.2268 - mean_absolute_error: 0.1535 - acc: 0.9164 - val_loss: 0.3507 - val_mean_absolute_error: 0.2068 - val_acc: 0.8364\n",
      "Epoch 68/128\n",
      "9830/9830 [==============================] - 6s 580us/step - loss: 0.2230 - mean_absolute_error: 0.1515 - acc: 0.9180 - val_loss: 0.3514 - val_mean_absolute_error: 0.2074 - val_acc: 0.8351\n",
      "Epoch 69/128\n",
      "9830/9830 [==============================] - 6s 578us/step - loss: 0.2269 - mean_absolute_error: 0.1553 - acc: 0.9125 - val_loss: 0.3495 - val_mean_absolute_error: 0.2068 - val_acc: 0.8360\n",
      "Epoch 70/128\n",
      "9830/9830 [==============================] - 6s 576us/step - loss: 0.2215 - mean_absolute_error: 0.1514 - acc: 0.9166 - val_loss: 0.3415 - val_mean_absolute_error: 0.2048 - val_acc: 0.8387\n",
      "Epoch 71/128\n",
      "9830/9830 [==============================] - 6s 575us/step - loss: 0.2142 - mean_absolute_error: 0.1476 - acc: 0.9204 - val_loss: 0.3398 - val_mean_absolute_error: 0.2018 - val_acc: 0.8438\n",
      "Epoch 72/128\n",
      "9830/9830 [==============================] - 6s 576us/step - loss: 0.2083 - mean_absolute_error: 0.1441 - acc: 0.9223 - val_loss: 0.3448 - val_mean_absolute_error: 0.1988 - val_acc: 0.8441\n",
      "Epoch 73/128\n",
      "9830/9830 [==============================] - 6s 617us/step - loss: 0.2077 - mean_absolute_error: 0.1390 - acc: 0.9226 - val_loss: 0.3622 - val_mean_absolute_error: 0.1984 - val_acc: 0.8410\n",
      "Epoch 74/128\n",
      "9830/9830 [==============================] - 6s 655us/step - loss: 0.2048 - mean_absolute_error: 0.1365 - acc: 0.9237 - val_loss: 0.3578 - val_mean_absolute_error: 0.1991 - val_acc: 0.8425\n",
      "Epoch 75/128\n",
      "9830/9830 [==============================] - 6s 596us/step - loss: 0.1996 - mean_absolute_error: 0.1355 - acc: 0.9254 - val_loss: 0.3509 - val_mean_absolute_error: 0.1962 - val_acc: 0.8421\n",
      "Epoch 76/128\n",
      "9830/9830 [==============================] - 6s 583us/step - loss: 0.1950 - mean_absolute_error: 0.1319 - acc: 0.9300 - val_loss: 0.3519 - val_mean_absolute_error: 0.1946 - val_acc: 0.8418\n",
      "Epoch 77/128\n",
      "9830/9830 [==============================] - 6s 590us/step - loss: 0.1903 - mean_absolute_error: 0.1290 - acc: 0.9298 - val_loss: 0.3487 - val_mean_absolute_error: 0.1936 - val_acc: 0.8444\n",
      "Epoch 78/128\n",
      "9830/9830 [==============================] - 6s 591us/step - loss: 0.1850 - mean_absolute_error: 0.1258 - acc: 0.9346 - val_loss: 0.3509 - val_mean_absolute_error: 0.1917 - val_acc: 0.8453\n",
      "Epoch 79/128\n",
      "9830/9830 [==============================] - 6s 606us/step - loss: 0.1829 - mean_absolute_error: 0.1222 - acc: 0.9334 - val_loss: 0.3504 - val_mean_absolute_error: 0.1897 - val_acc: 0.8477\n",
      "Epoch 80/128\n",
      "9830/9830 [==============================] - 6s 571us/step - loss: 0.1771 - mean_absolute_error: 0.1191 - acc: 0.9359 - val_loss: 0.3490 - val_mean_absolute_error: 0.1889 - val_acc: 0.8473\n",
      "Epoch 81/128\n",
      "9830/9830 [==============================] - 6s 566us/step - loss: 0.1796 - mean_absolute_error: 0.1192 - acc: 0.9332 - val_loss: 0.3531 - val_mean_absolute_error: 0.1873 - val_acc: 0.8493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/128\n",
      "9830/9830 [==============================] - 6s 572us/step - loss: 0.1741 - mean_absolute_error: 0.1148 - acc: 0.9358 - val_loss: 0.3606 - val_mean_absolute_error: 0.1869 - val_acc: 0.8477\n",
      "Epoch 83/128\n",
      "9830/9830 [==============================] - 6s 579us/step - loss: 0.1729 - mean_absolute_error: 0.1137 - acc: 0.9361 - val_loss: 0.3628 - val_mean_absolute_error: 0.1865 - val_acc: 0.8471\n",
      "Epoch 84/128\n",
      "9830/9830 [==============================] - 6s 576us/step - loss: 0.1681 - mean_absolute_error: 0.1100 - acc: 0.9404 - val_loss: 0.3692 - val_mean_absolute_error: 0.1869 - val_acc: 0.8459\n",
      "Epoch 85/128\n",
      "9830/9830 [==============================] - 6s 569us/step - loss: 0.1588 - mean_absolute_error: 0.1068 - acc: 0.9434 - val_loss: 0.3731 - val_mean_absolute_error: 0.1878 - val_acc: 0.8464\n",
      "Epoch 86/128\n",
      "9830/9830 [==============================] - 6s 575us/step - loss: 0.1661 - mean_absolute_error: 0.1099 - acc: 0.9389 - val_loss: 0.3826 - val_mean_absolute_error: 0.1895 - val_acc: 0.8421\n",
      "Epoch 87/128\n",
      "9830/9830 [==============================] - 6s 588us/step - loss: 0.1576 - mean_absolute_error: 0.1047 - acc: 0.9436 - val_loss: 0.3868 - val_mean_absolute_error: 0.1878 - val_acc: 0.8427\n",
      "Epoch 88/128\n",
      "9830/9830 [==============================] - 6s 573us/step - loss: 0.1575 - mean_absolute_error: 0.1029 - acc: 0.9451 - val_loss: 0.3860 - val_mean_absolute_error: 0.1860 - val_acc: 0.8441\n",
      "Epoch 89/128\n",
      "9830/9830 [==============================] - 6s 573us/step - loss: 0.1570 - mean_absolute_error: 0.1031 - acc: 0.9437 - val_loss: 0.3833 - val_mean_absolute_error: 0.1861 - val_acc: 0.8459\n",
      "Epoch 90/128\n",
      "9830/9830 [==============================] - 6s 589us/step - loss: 0.1522 - mean_absolute_error: 0.1010 - acc: 0.9455 - val_loss: 0.3775 - val_mean_absolute_error: 0.1844 - val_acc: 0.8445\n",
      "Epoch 91/128\n",
      "9830/9830 [==============================] - 6s 581us/step - loss: 0.1442 - mean_absolute_error: 0.0962 - acc: 0.9496 - val_loss: 0.3756 - val_mean_absolute_error: 0.1830 - val_acc: 0.8460\n",
      "Epoch 92/128\n",
      "9830/9830 [==============================] - 6s 585us/step - loss: 0.1488 - mean_absolute_error: 0.0983 - acc: 0.9477 - val_loss: 0.3900 - val_mean_absolute_error: 0.1857 - val_acc: 0.8419\n",
      "Epoch 93/128\n",
      "9830/9830 [==============================] - 6s 583us/step - loss: 0.1430 - mean_absolute_error: 0.0960 - acc: 0.9515 - val_loss: 0.3900 - val_mean_absolute_error: 0.1840 - val_acc: 0.8433\n",
      "Epoch 94/128\n",
      "9830/9830 [==============================] - 6s 583us/step - loss: 0.1461 - mean_absolute_error: 0.0947 - acc: 0.9477 - val_loss: 0.4203 - val_mean_absolute_error: 0.1851 - val_acc: 0.8360\n",
      "Epoch 95/128\n",
      "9830/9830 [==============================] - 6s 569us/step - loss: 0.1489 - mean_absolute_error: 0.0946 - acc: 0.9440 - val_loss: 0.4096 - val_mean_absolute_error: 0.1841 - val_acc: 0.8418\n",
      "Epoch 96/128\n",
      "9830/9830 [==============================] - 6s 590us/step - loss: 0.1401 - mean_absolute_error: 0.0906 - acc: 0.9527 - val_loss: 0.4268 - val_mean_absolute_error: 0.1886 - val_acc: 0.8346\n",
      "Epoch 97/128\n",
      "9830/9830 [==============================] - 6s 593us/step - loss: 0.1446 - mean_absolute_error: 0.0938 - acc: 0.9482 - val_loss: 0.3926 - val_mean_absolute_error: 0.1816 - val_acc: 0.8450\n",
      "Epoch 98/128\n",
      "9830/9830 [==============================] - 6s 606us/step - loss: 0.1376 - mean_absolute_error: 0.0903 - acc: 0.9513 - val_loss: 0.3863 - val_mean_absolute_error: 0.1798 - val_acc: 0.8438\n",
      "Epoch 99/128\n",
      "9830/9830 [==============================] - 6s 593us/step - loss: 0.1363 - mean_absolute_error: 0.0890 - acc: 0.9515 - val_loss: 0.3840 - val_mean_absolute_error: 0.1782 - val_acc: 0.8451\n",
      "Epoch 100/128\n",
      "9830/9830 [==============================] - 6s 570us/step - loss: 0.1387 - mean_absolute_error: 0.0875 - acc: 0.9508 - val_loss: 0.3986 - val_mean_absolute_error: 0.1806 - val_acc: 0.8435\n",
      "Epoch 101/128\n",
      "9830/9830 [==============================] - 6s 568us/step - loss: 0.1275 - mean_absolute_error: 0.0834 - acc: 0.9570 - val_loss: 0.4132 - val_mean_absolute_error: 0.1823 - val_acc: 0.8416\n",
      "Epoch 102/128\n",
      "9830/9830 [==============================] - 6s 572us/step - loss: 0.1280 - mean_absolute_error: 0.0835 - acc: 0.9576 - val_loss: 0.4057 - val_mean_absolute_error: 0.1795 - val_acc: 0.8450\n",
      "Epoch 103/128\n",
      "9830/9830 [==============================] - 6s 595us/step - loss: 0.1206 - mean_absolute_error: 0.0802 - acc: 0.9596 - val_loss: 0.4021 - val_mean_absolute_error: 0.1779 - val_acc: 0.8456\n",
      "Epoch 104/128\n",
      "9830/9830 [==============================] - 6s 586us/step - loss: 0.1194 - mean_absolute_error: 0.0770 - acc: 0.9606 - val_loss: 0.4256 - val_mean_absolute_error: 0.1803 - val_acc: 0.8415\n",
      "Epoch 105/128\n",
      "9830/9830 [==============================] - 6s 572us/step - loss: 0.1189 - mean_absolute_error: 0.0770 - acc: 0.9596 - val_loss: 0.4250 - val_mean_absolute_error: 0.1800 - val_acc: 0.8428\n",
      "Epoch 106/128\n",
      "9830/9830 [==============================] - 6s 571us/step - loss: 0.1166 - mean_absolute_error: 0.0757 - acc: 0.9608 - val_loss: 0.4196 - val_mean_absolute_error: 0.1782 - val_acc: 0.8433\n",
      "Epoch 107/128\n",
      "9830/9830 [==============================] - 6s 571us/step - loss: 0.1168 - mean_absolute_error: 0.0749 - acc: 0.9609 - val_loss: 0.4245 - val_mean_absolute_error: 0.1781 - val_acc: 0.8445\n",
      "Epoch 108/128\n",
      "9830/9830 [==============================] - 6s 568us/step - loss: 0.1192 - mean_absolute_error: 0.0777 - acc: 0.9584 - val_loss: 0.4184 - val_mean_absolute_error: 0.1830 - val_acc: 0.8404\n",
      "Epoch 109/128\n",
      "9830/9830 [==============================] - 6s 569us/step - loss: 0.1309 - mean_absolute_error: 0.0844 - acc: 0.9540 - val_loss: 0.4067 - val_mean_absolute_error: 0.1800 - val_acc: 0.8427\n",
      "Epoch 110/128\n",
      "9830/9830 [==============================] - 6s 593us/step - loss: 0.1266 - mean_absolute_error: 0.0821 - acc: 0.9568 - val_loss: 0.4096 - val_mean_absolute_error: 0.1770 - val_acc: 0.8450\n",
      "Epoch 111/128\n",
      "9830/9830 [==============================] - 6s 595us/step - loss: 0.1250 - mean_absolute_error: 0.0790 - acc: 0.9588 - val_loss: 0.4104 - val_mean_absolute_error: 0.1761 - val_acc: 0.8460\n",
      "Epoch 112/128\n",
      "9830/9830 [==============================] - 6s 602us/step - loss: 0.1213 - mean_absolute_error: 0.0782 - acc: 0.9581 - val_loss: 0.4171 - val_mean_absolute_error: 0.1780 - val_acc: 0.8413\n",
      "Epoch 113/128\n",
      "9830/9830 [==============================] - 6s 581us/step - loss: 0.1186 - mean_absolute_error: 0.0767 - acc: 0.9638 - val_loss: 0.4046 - val_mean_absolute_error: 0.1748 - val_acc: 0.8474\n",
      "Epoch 114/128\n",
      "9830/9830 [==============================] - 6s 590us/step - loss: 0.1186 - mean_absolute_error: 0.0767 - acc: 0.9582 - val_loss: 0.4303 - val_mean_absolute_error: 0.1784 - val_acc: 0.8392\n",
      "Epoch 115/128\n",
      "9830/9830 [==============================] - 6s 575us/step - loss: 0.1130 - mean_absolute_error: 0.0730 - acc: 0.9630 - val_loss: 0.4256 - val_mean_absolute_error: 0.1770 - val_acc: 0.8422\n",
      "Epoch 116/128\n",
      "9830/9830 [==============================] - 6s 599us/step - loss: 0.1105 - mean_absolute_error: 0.0720 - acc: 0.9648 - val_loss: 0.4270 - val_mean_absolute_error: 0.1767 - val_acc: 0.8428\n",
      "Epoch 117/128\n",
      "9830/9830 [==============================] - 6s 606us/step - loss: 0.1075 - mean_absolute_error: 0.0701 - acc: 0.9644 - val_loss: 0.4193 - val_mean_absolute_error: 0.1742 - val_acc: 0.8464\n",
      "Epoch 118/128\n",
      "9830/9830 [==============================] - 6s 576us/step - loss: 0.1052 - mean_absolute_error: 0.0682 - acc: 0.9651 - val_loss: 0.4266 - val_mean_absolute_error: 0.1742 - val_acc: 0.8456\n",
      "Epoch 119/128\n",
      "9830/9830 [==============================] - 6s 575us/step - loss: 0.1037 - mean_absolute_error: 0.0653 - acc: 0.9651 - val_loss: 0.4378 - val_mean_absolute_error: 0.1752 - val_acc: 0.8421\n",
      "Epoch 120/128\n",
      "9830/9830 [==============================] - 6s 594us/step - loss: 0.1002 - mean_absolute_error: 0.0638 - acc: 0.9671 - val_loss: 0.4336 - val_mean_absolute_error: 0.1734 - val_acc: 0.8447\n",
      "Epoch 121/128\n",
      "9830/9830 [==============================] - 6s 582us/step - loss: 0.1038 - mean_absolute_error: 0.0643 - acc: 0.9662 - val_loss: 0.4316 - val_mean_absolute_error: 0.1719 - val_acc: 0.8476\n",
      "Epoch 122/128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9830/9830 [==============================] - 6s 569us/step - loss: 0.1029 - mean_absolute_error: 0.0639 - acc: 0.9663 - val_loss: 0.4375 - val_mean_absolute_error: 0.1727 - val_acc: 0.8465\n",
      "Epoch 123/128\n",
      "9830/9830 [==============================] - 6s 569us/step - loss: 0.1004 - mean_absolute_error: 0.0625 - acc: 0.9661 - val_loss: 0.4545 - val_mean_absolute_error: 0.1752 - val_acc: 0.8431\n",
      "Epoch 124/128\n",
      "9830/9830 [==============================] - 6s 592us/step - loss: 0.0970 - mean_absolute_error: 0.0605 - acc: 0.9683 - val_loss: 0.4604 - val_mean_absolute_error: 0.1746 - val_acc: 0.8407\n",
      "Epoch 125/128\n",
      "9830/9830 [==============================] - 6s 642us/step - loss: 0.1025 - mean_absolute_error: 0.0617 - acc: 0.9653 - val_loss: 0.4524 - val_mean_absolute_error: 0.1723 - val_acc: 0.8425\n",
      "Epoch 126/128\n",
      "9830/9830 [==============================] - 6s 573us/step - loss: 0.0959 - mean_absolute_error: 0.0590 - acc: 0.9679 - val_loss: 0.4599 - val_mean_absolute_error: 0.1735 - val_acc: 0.8403\n",
      "Epoch 127/128\n",
      "9830/9830 [==============================] - 6s 570us/step - loss: 0.0975 - mean_absolute_error: 0.0592 - acc: 0.9693 - val_loss: 0.4560 - val_mean_absolute_error: 0.1732 - val_acc: 0.8424\n",
      "Epoch 128/128\n",
      "9830/9830 [==============================] - 6s 572us/step - loss: 0.0960 - mean_absolute_error: 0.0584 - acc: 0.9687 - val_loss: 0.4693 - val_mean_absolute_error: 0.1752 - val_acc: 0.8399\n"
     ]
    }
   ],
   "source": [
    "print('model training process.')\n",
    "history = model.fit(x_train, y_train, batch_size=training_batch_size, epochs=epochs, validation_split=validation_split)\n",
    "\n",
    "dt = datetime.datetime.now()\n",
    "dt = str(dt).replace(':', '_').replace(' ', '_').replace('-', '_')\n",
    "\n",
    "with open('../training/history/%s_log_%s_%s.json' % (current_model, dt, batch_size), 'w') as fp:\n",
    "    json.dump(history.history, fp)\n",
    "\n",
    "model.save('../model/%s_compare_model_%s_%s.h5' % (current_model, dt, batch_size))\n",
    "\n",
    "# test = get_dataset_set(dataset_paths, batch_size=32, samples_per_file=5)\n",
    "# y_test = test[1]\n",
    "# x_test = [test[0][0], test[0][1]]\n",
    "\n",
    "# model prediction\n",
    "class_names = ['Similar', 'Dissimilar']\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "positive_predictions = []\n",
    "negative_predictions = []\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if(y_test[i][0] == 1.):\n",
    "        positive_predictions.append(predictions[i][0])\n",
    "    else:\n",
    "        negative_predictions.append(predictions[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histogram of prediction values\n",
    "n_bins = 10\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "axs[0].hist(positive_predictions, n_bins)\n",
    "axs[0].set_title('Positive')\n",
    "axs[1].hist(negative_predictions, n_bins)\n",
    "axs[1].set_title('Negative')\n",
    "plt.savefig('../training/distribution/%s_pred_dist_%s_%s.png' % (current_model, dt, batch_size))\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[1901  149]\n",
      " [ 528 1518]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x192e8d3dba8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training history\n",
    "acc = [history.history['acc'], history.history['val_acc']]\n",
    "plot_training_history(acc, save_path='../training/acc/%s_acc_%s_%s.png' % (current_model, dt, batch_size), yl='Accuracy',)\n",
    "\n",
    "loss = [history.history['loss'], history.history['val_loss']]\n",
    "plot_training_history(loss, save_path='../training/loss/%s_loss_%s_%s.png' % (current_model, dt, batch_size), yl='Loss (Binary Crossentropy)')\n",
    "\n",
    "error = [history.history['mean_absolute_error'], history.history['val_mean_absolute_error']]\n",
    "plot_training_history(error, save_path='../training/error/%s_error_%s_%s.png' % (current_model, dt, batch_size), yl='MAE')\n",
    "\n",
    "# confusion matrix of actual and predict\n",
    "plot_confusion_matrix(y_test >= .5, predictions >= .5, class_names, save_path='../training/cm/%s_cm_%s_%s.png' % (current_model, dt, batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa_venv",
   "language": "python",
   "name": "qa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
