{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# utils\n",
    "import datetime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# numerical libs\n",
    "import numpy as np\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import BatchNormalization, Bidirectional, Concatenate, Dense, Flatten, GRU, Input, Lambda, LSTM, Masking, multiply, Permute, RepeatVector\n",
    "from keras.models import load_model, Model, Sequential\n",
    "\n",
    "# seed and precision\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# file i/o paths\n",
    "q_path = '../data/dataset/questions/embedded_questions_4000_40_300.npy'\n",
    "n0_path = 'D:/Users/Patdanai/th-qasys-db/n0_embedded/n0_embedded/'\n",
    "n1_path = 'D:/Users/Patdanai/th-qasys-db/n1_embedded/n1_embedded/'\n",
    "p_path = 'D:/Users/Patdanai/th-qasys-db/positive_embedded/positive_embedded/'\n",
    "dataset_paths = [p_path, n0_path, n1_path, q_path]\n",
    "\n",
    "# model hyperparameters\n",
    "hidden_nodes = 16\n",
    "rnn_units = 64\n",
    "sl = 40\n",
    "wvl = 300\n",
    "\n",
    "def attention_layer(inputs, time_step):\n",
    "    a = Lambda(lambda x: x, output_shape=lambda s: s)(inputs)\n",
    "    a = Permute((2, 1))(a)\n",
    "    a = Dense(time_step, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    output_attention = multiply([inputs, a_probs], name='multiply_attention')\n",
    "    return output_attention\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    beta = 1\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "def get_input(input_path):\n",
    "    return np.load(input_path)\n",
    "\n",
    "def get_dataset_set(dataset_paths, batch_size=4000, samples_per_file=5):\n",
    "    if(samples_per_file > 5 or not samples_per_file):\n",
    "        print('samples per file must be in range [0, 5]')\n",
    "        return\n",
    "\n",
    "    question_sentences = get_input(dataset_paths[3])\n",
    "    print(question_sentences.shape)\n",
    "\n",
    "    files = np.random.permutation(os.listdir(dataset_paths[0]))\n",
    "    \n",
    "    s_seq = (0, sl, wvl)\n",
    "    l_shape = (0, 1)\n",
    "\n",
    "    questions_batch = np.empty((s_seq))\n",
    "    sentences_batch = np.empty((s_seq))\n",
    "    labels_batch = np.empty((l_shape))\n",
    "\n",
    "    f_count = 0\n",
    "    for f_name in files:\n",
    "        if(f_count > batch_size - 1):\n",
    "            break\n",
    "\n",
    "        q_idx = f_name.replace('positive_question', '').replace('.npy', '')\n",
    "\n",
    "        q = question_sentences[int(q_idx)]\n",
    "        q = np.expand_dims(q, axis=0)\n",
    "\n",
    "        positive_s = get_input(dataset_paths[0] + f_name)[:samples_per_file]\n",
    "        l = np.ones((positive_s.shape[0], 1))\n",
    "        labels_batch = np.concatenate((labels_batch, l), axis=0)\n",
    "\n",
    "        negative_s = np.empty((s_seq))\n",
    "        if(samples_per_file < 2):\n",
    "            r = np.random.randint(2)\n",
    "            if(r):\n",
    "                negative0_s = get_input(dataset_paths[1] + 'negative0_question%s.npy' % q_idx)[:1]\n",
    "                negative_s = np.concatenate((negative_s, negative0_s), axis=0)\n",
    "            else:\n",
    "                negative1_s = get_input(dataset_paths[2] + 'negative1_question%s.npy' % q_idx)[:1]\n",
    "                negative_s = np.concatenate((negative_s, negative1_s), axis=0)\n",
    "        else:\n",
    "            n0_instance = samples_per_file // 2\n",
    "            negative0_s = get_input(dataset_paths[1] + 'negative0_question%s.npy' % q_idx)[:n0_instance]\n",
    "            n1_instance = samples_per_file - negative0_s.shape[0]\n",
    "            negative1_s = get_input(dataset_paths[2] + 'negative1_question%s.npy' % q_idx)[:n1_instance]\n",
    "            \n",
    "            temp = np.concatenate((negative0_s, negative1_s), axis=0)\n",
    "            negative_s = np.concatenate((negative_s, temp), axis=0)\n",
    "        \n",
    "        temp_s = np.concatenate((positive_s, negative_s), axis=0)\n",
    "        sentences_batch = np.concatenate((sentences_batch, temp_s), axis=0)\n",
    "        l = np.zeros((negative_s.shape[0], 1))\n",
    "        labels_batch = np.concatenate((labels_batch, l), axis=0)\n",
    "\n",
    "        temp_q = np.repeat(q, positive_s.shape[0] + negative_s.shape[0], axis=0)\n",
    "        questions_batch = np.concatenate((questions_batch, temp_q), axis=0)\n",
    "\n",
    "        f_count += 1\n",
    "        \n",
    "        if(f_count % 100):\n",
    "            print('file[%s/%s] => ps_shape = %s ns_shape = %s ps_ns_shape = %s l_shape = %s qs_shape = %s' % (f_count, batch_size, positive_s.shape, negative_s.shape, sentences_batch.shape, labels_batch.shape, questions_batch.shape))\n",
    "    \n",
    "    return ([questions_batch, sentences_batch], labels_batch)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, cmap=plt.cm.Blues,\n",
    "                        normalize=0, save_path=None, title=None, verbose=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting ``normalize=True``.\n",
    "    \"\"\"\n",
    "\n",
    "    if(not title):\n",
    "        if(normalize):\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if(normalize):\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    if(verbose):\n",
    "        print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='Actual',\n",
    "           xlabel='Predicted')\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"right\",\n",
    "            rotation_mode=\"anchor\")\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if(save_path):\n",
    "        plt.savefig(save_path)\n",
    "        plt.clf()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    return ax\n",
    "\n",
    "def plot_training_history(h, labels=['Training', 'Validation'], yl=None, save_path=None):\n",
    "    plt.plot(h[0], label=labels[0])\n",
    "    plt.plot(h[1], '-r', label=labels[1])\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('Epochs')\n",
    "\n",
    "    if(yl):\n",
    "        plt.ylabel(yl)\n",
    "    \n",
    "    if(save_path):\n",
    "        plt.savefig(save_path)\n",
    "        plt.clf()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def sequence_generator(files, label_path, batch_size=512):\n",
    "    # files => os.listdir\n",
    "    if(label_path):\n",
    "        positive_dataset_path = label_path[0]\n",
    "        negative0_dataset_path = label_path[1]\n",
    "        negative1_dataset_path = label_path[2]\n",
    "    \n",
    "    while(1):\n",
    "        batch_paths = np.random.choice(files, size=batch_size)\n",
    "        input_batch = np.empty((0, 20, 300))\n",
    "        output_batch = np.empty((0, 1))\n",
    "\n",
    "        inp = None\n",
    "        for input_path in batch_paths:\n",
    "            if('positive' in input_path):\n",
    "                inp = get_input(positive_dataset_path + input_path)\n",
    "                out = np.ones((inp.shape[0], 1))\n",
    "            elif('negative0' in input_path):\n",
    "                inp = get_input(negative0_dataset_path + input_path)\n",
    "                out = np.zeros((inp.shape[0], 1))\n",
    "            elif('negative1' in input_path):\n",
    "                inp = get_input(negative1_dataset_path + input_path)\n",
    "                out = np.zeros((inp.shape[0], 1))\n",
    "\n",
    "            input_batch = np.concatenate((input_batch, inp), axis=0)\n",
    "            output_batch = np.concatenate((output_batch, out), axis=0)\n",
    "\n",
    "        input_batch = np.array(input_batch)\n",
    "        output_batch = np.array(output_batch)\n",
    "\n",
    "        yield (input_batch, output_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 40, 300)\n"
     ]
    }
   ],
   "source": [
    "##   dataset\n",
    "batch_size = 2048\n",
    "samples_per_file = 5\n",
    "validation_proportion = .8\n",
    "\n",
    "##    model fitting\n",
    "epochs = 64\n",
    "training_batch_size = 2048\n",
    "validation_split = .4\n",
    "\n",
    "# generate training set\n",
    "training_set = get_dataset_set(dataset_paths, batch_size=batch_size, samples_per_file=samples_per_file)\n",
    "\n",
    "training_size = int(len(training_set[1]) * validation_proportion)\n",
    "y_train = training_set[1][:training_size]\n",
    "x_train = [training_set[0][0][:training_size], training_set[0][1][:training_size]]\n",
    "\n",
    "y_test = training_set[1][training_size:]\n",
    "x_test = [training_set[0][0][training_size:], training_set[0][1][training_size:]]\n",
    "\n",
    "print('train on', len(y_train), 'samples')\n",
    "print('validate on', len(y_test), 'samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(recurrent_layer):\n",
    "    input_seq = Input(shape=(sl, wvl))\n",
    "    masking = Masking(mask_value=0., input_shape=(sl, wvl))(input_seq)\n",
    "    rl = recurrent_layer(masking)\n",
    "    multiply_attention = attention_layer(rl, sl)\n",
    "    multiply_attention = Lambda(lambda x: x, output_shape=lambda s: s)(multiply_attention)\n",
    "    multiply_attention = Flatten()(multiply_attention)\n",
    "    output = Dense(rnn_units, activation='relu')(multiply_attention)\n",
    "    submodel = Model(inputs=input_seq, outputs=output)\n",
    "    submodel.summary()\n",
    "    return submodel\n",
    "\n",
    "def sentence_compare():\n",
    "    qv = Input(shape=(rnn_units, ), name='vectorized_q')\n",
    "    sv = Input(shape=(rnn_units, ), name='vectorized_s')\n",
    "\n",
    "    concatenate = Concatenate()([qv, sv])\n",
    "    dense1 = Dense(hidden_nodes, activation='sigmoid')(concatenate)\n",
    "    dense2 = Dense(hidden_nodes, activation='sigmoid')(dense1)\n",
    "    similarity = Dense(1, activation='sigmoid')(dense2)\n",
    "    submodel = Model(inputs=[qv, sv], outputs=similarity)\n",
    "    submodel.summary()\n",
    "    return submodel\n",
    "\n",
    "\n",
    "current_model = 'lstm'\n",
    "# create tensors \n",
    "q_seq = Input(shape=(sl, wvl), name='question_seq')\n",
    "s_seq = Input(shape=(sl, wvl), name='sentence_seq')\n",
    "\n",
    "# create recurrrent layers\n",
    "rl1 = Bidirectional(LSTM(rnn_units, activation='relu', dropout=.4, recurrent_dropout=.1, return_sequences=1))\n",
    "rl2 = Bidirectional(LSTM(rnn_units, activation='relu', dropout=.4, recurrent_dropout=.1, return_sequences=1))\n",
    "\n",
    "# create output tensors\n",
    "qv = sentence_vector(rl1)\n",
    "qv.name = 'question_vector'\n",
    "qv = qv(q_seq)\n",
    "\n",
    "sv = sentence_vector(rl2)\n",
    "sv.name = 'sentence_vector'\n",
    "sv = sv(s_seq)\n",
    "\n",
    "similarity = sentence_compare()\n",
    "similarity.name = 'compare'\n",
    "similarity = similarity([qv, sv])\n",
    "\n",
    "# form model\n",
    "model = Model(inputs=[q_seq, s_seq], outputs=similarity)\n",
    "model.summary()\n",
    "\n",
    "# training configuration\n",
    "##    model confiuration\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mae', 'acc'])\n",
    "\n",
    "##    load model\n",
    "# model = load_model('../model/trained_compare_model_2019_05_03_14_55_55.705298.h5')\n",
    "\n",
    "print('model training process.')\n",
    "history = model.fit(x_train, y_train, batch_size=training_batch_size, epochs=epochs, validation_split=validation_split)\n",
    "\n",
    "dt = datetime.datetime.now()\n",
    "dt = str(dt).replace(':', '_').replace(' ', '_').replace('-', '_')\n",
    "\n",
    "with open('../training/history/%s_log_%s_%s.json' % (current_model, dt, batch_size), 'w') as fp:\n",
    "    json.dump(history.history, fp)\n",
    "\n",
    "model.save('../model/%s_compare_model_%s_%s.h5' % (current_model, dt, batch_size))\n",
    "\n",
    "# test = get_dataset_set(dataset_paths, batch_size=32, samples_per_file=5)\n",
    "# y_test = test[1]\n",
    "# x_test = [test[0][0], test[0][1]]\n",
    "\n",
    "# model prediction\n",
    "class_names = ['Similar', 'Dissimilar']\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "positive_predictions = []\n",
    "negative_predictions = []\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if(y_test[i][0] == 1.):\n",
    "        positive_predictions.append(predictions[i][0])\n",
    "    else:\n",
    "        negative_predictions.append(predictions[i][0])\n",
    "\n",
    "# plot histogram of prediction values\n",
    "n_bins = 10\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "axs[0].hist(positive_predictions, n_bins)\n",
    "axs[0].set_title('Positive')\n",
    "axs[1].hist(negative_predictions, n_bins)\n",
    "axs[1].set_title('Negative')\n",
    "plt.savefig('../training/distribution/%s_pred_dist_%s_%s.png' % (current_model, dt, batch_size))\n",
    "plt.clf()\n",
    "\n",
    "# plot training history\n",
    "acc = [history.history['acc'], history.history['val_acc']]\n",
    "plot_training_history(acc, save_path='../training/acc/%s_acc_%s_%s.png' % (current_model, dt, batch_size), yl='Accuracy',)\n",
    "\n",
    "loss = [history.history['loss'], history.history['val_loss']]\n",
    "plot_training_history(loss, save_path='../training/loss/%s_loss_%s_%s.png' % (current_model, dt, batch_size), yl='Loss (Binary Crossentropy)')\n",
    "\n",
    "error = [history.history['mean_absolute_error'], history.history['val_mean_absolute_error']]\n",
    "plot_training_history(error, save_path='../training/error/%s_error_%s_%s.png' % (current_model, dt, batch_size), yl='MAE')\n",
    "\n",
    "# confusion matrix of actual and predict\n",
    "plot_confusion_matrix(y_test >= .5, predictions >= .5, class_names, save_path='../training/cm/%s_cm_%s_%s.png' % (current_model, dt, batch_size))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa_venv",
   "language": "python",
   "name": "qa_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
